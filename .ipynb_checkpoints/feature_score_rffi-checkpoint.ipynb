{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1291afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline   \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skrebate import ReliefF\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC\n",
    "from xgboost.sklearn import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0662d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vuduc\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1597: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "C:\\Users\\vuduc\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SLC39A8</th>\n",
       "      <th>RHAG</th>\n",
       "      <th>DDIT4</th>\n",
       "      <th>MPO</th>\n",
       "      <th>RRM2</th>\n",
       "      <th>CCL3</th>\n",
       "      <th>TGFBI</th>\n",
       "      <th>MAFF</th>\n",
       "      <th>TYMS</th>\n",
       "      <th>ENPP2</th>\n",
       "      <th>...</th>\n",
       "      <th>FEN1</th>\n",
       "      <th>AURKA</th>\n",
       "      <th>FRAT1</th>\n",
       "      <th>SNRPG</th>\n",
       "      <th>KIF2C</th>\n",
       "      <th>POLE2</th>\n",
       "      <th>UBE2C</th>\n",
       "      <th>AURKB</th>\n",
       "      <th>CENPF</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.837760</td>\n",
       "      <td>4.112866</td>\n",
       "      <td>8.046405</td>\n",
       "      <td>2.324069</td>\n",
       "      <td>4.759059</td>\n",
       "      <td>3.140721</td>\n",
       "      <td>4.143039</td>\n",
       "      <td>8.578116</td>\n",
       "      <td>5.230842</td>\n",
       "      <td>3.862937</td>\n",
       "      <td>...</td>\n",
       "      <td>4.062130</td>\n",
       "      <td>3.009849</td>\n",
       "      <td>8.785775</td>\n",
       "      <td>9.787393</td>\n",
       "      <td>2.240854</td>\n",
       "      <td>2.622261</td>\n",
       "      <td>4.787508</td>\n",
       "      <td>3.002504</td>\n",
       "      <td>2.246339</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.751983</td>\n",
       "      <td>1.485795</td>\n",
       "      <td>8.971049</td>\n",
       "      <td>5.258546</td>\n",
       "      <td>7.684430</td>\n",
       "      <td>2.811141</td>\n",
       "      <td>4.977650</td>\n",
       "      <td>4.715507</td>\n",
       "      <td>7.567845</td>\n",
       "      <td>3.212268</td>\n",
       "      <td>...</td>\n",
       "      <td>4.651380</td>\n",
       "      <td>4.084429</td>\n",
       "      <td>8.823993</td>\n",
       "      <td>9.877403</td>\n",
       "      <td>3.083158</td>\n",
       "      <td>2.470562</td>\n",
       "      <td>5.049623</td>\n",
       "      <td>2.667229</td>\n",
       "      <td>3.301098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.279053</td>\n",
       "      <td>1.925620</td>\n",
       "      <td>8.712714</td>\n",
       "      <td>1.856427</td>\n",
       "      <td>5.890788</td>\n",
       "      <td>5.947866</td>\n",
       "      <td>4.036440</td>\n",
       "      <td>9.109963</td>\n",
       "      <td>6.066830</td>\n",
       "      <td>3.086101</td>\n",
       "      <td>...</td>\n",
       "      <td>3.554817</td>\n",
       "      <td>2.921564</td>\n",
       "      <td>8.135534</td>\n",
       "      <td>10.673182</td>\n",
       "      <td>2.399208</td>\n",
       "      <td>2.270243</td>\n",
       "      <td>4.136699</td>\n",
       "      <td>3.181530</td>\n",
       "      <td>2.663032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.601609</td>\n",
       "      <td>5.888388</td>\n",
       "      <td>8.527441</td>\n",
       "      <td>5.750055</td>\n",
       "      <td>8.206514</td>\n",
       "      <td>5.134266</td>\n",
       "      <td>4.616064</td>\n",
       "      <td>5.156271</td>\n",
       "      <td>8.057016</td>\n",
       "      <td>2.684719</td>\n",
       "      <td>...</td>\n",
       "      <td>5.330526</td>\n",
       "      <td>4.034944</td>\n",
       "      <td>7.884369</td>\n",
       "      <td>9.025149</td>\n",
       "      <td>3.205451</td>\n",
       "      <td>2.998327</td>\n",
       "      <td>5.846744</td>\n",
       "      <td>3.883344</td>\n",
       "      <td>3.662963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.785195</td>\n",
       "      <td>4.846312</td>\n",
       "      <td>9.301229</td>\n",
       "      <td>8.695507</td>\n",
       "      <td>10.032503</td>\n",
       "      <td>6.555199</td>\n",
       "      <td>6.542890</td>\n",
       "      <td>8.041716</td>\n",
       "      <td>9.813448</td>\n",
       "      <td>7.125344</td>\n",
       "      <td>...</td>\n",
       "      <td>6.783366</td>\n",
       "      <td>6.314857</td>\n",
       "      <td>8.603358</td>\n",
       "      <td>10.328848</td>\n",
       "      <td>4.966912</td>\n",
       "      <td>4.004817</td>\n",
       "      <td>6.864789</td>\n",
       "      <td>5.244933</td>\n",
       "      <td>4.199692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>7.181331</td>\n",
       "      <td>2.365467</td>\n",
       "      <td>8.184828</td>\n",
       "      <td>6.181315</td>\n",
       "      <td>7.871825</td>\n",
       "      <td>2.929050</td>\n",
       "      <td>7.998090</td>\n",
       "      <td>4.431776</td>\n",
       "      <td>7.034406</td>\n",
       "      <td>3.780909</td>\n",
       "      <td>...</td>\n",
       "      <td>4.477342</td>\n",
       "      <td>4.055716</td>\n",
       "      <td>8.839066</td>\n",
       "      <td>9.910092</td>\n",
       "      <td>2.871094</td>\n",
       "      <td>3.078237</td>\n",
       "      <td>3.940554</td>\n",
       "      <td>3.216972</td>\n",
       "      <td>3.462929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>5.251613</td>\n",
       "      <td>2.714796</td>\n",
       "      <td>6.427088</td>\n",
       "      <td>3.523980</td>\n",
       "      <td>7.221401</td>\n",
       "      <td>2.154800</td>\n",
       "      <td>4.979873</td>\n",
       "      <td>7.541867</td>\n",
       "      <td>7.252205</td>\n",
       "      <td>2.629904</td>\n",
       "      <td>...</td>\n",
       "      <td>5.162441</td>\n",
       "      <td>3.987030</td>\n",
       "      <td>9.600255</td>\n",
       "      <td>6.991776</td>\n",
       "      <td>2.501947</td>\n",
       "      <td>2.859321</td>\n",
       "      <td>4.154692</td>\n",
       "      <td>3.430309</td>\n",
       "      <td>3.199781</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>5.905524</td>\n",
       "      <td>8.074482</td>\n",
       "      <td>6.259182</td>\n",
       "      <td>2.492298</td>\n",
       "      <td>5.146876</td>\n",
       "      <td>1.794615</td>\n",
       "      <td>8.446067</td>\n",
       "      <td>8.769124</td>\n",
       "      <td>4.142922</td>\n",
       "      <td>2.246056</td>\n",
       "      <td>...</td>\n",
       "      <td>4.168839</td>\n",
       "      <td>4.630949</td>\n",
       "      <td>7.698138</td>\n",
       "      <td>9.373841</td>\n",
       "      <td>2.188718</td>\n",
       "      <td>2.145200</td>\n",
       "      <td>4.212061</td>\n",
       "      <td>2.516086</td>\n",
       "      <td>2.280285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>4.786756</td>\n",
       "      <td>2.279402</td>\n",
       "      <td>5.955421</td>\n",
       "      <td>8.199064</td>\n",
       "      <td>8.008226</td>\n",
       "      <td>3.352024</td>\n",
       "      <td>9.477510</td>\n",
       "      <td>5.653967</td>\n",
       "      <td>7.454457</td>\n",
       "      <td>2.954322</td>\n",
       "      <td>...</td>\n",
       "      <td>4.676477</td>\n",
       "      <td>4.053300</td>\n",
       "      <td>8.976000</td>\n",
       "      <td>6.692781</td>\n",
       "      <td>2.799296</td>\n",
       "      <td>2.676570</td>\n",
       "      <td>4.434418</td>\n",
       "      <td>3.215429</td>\n",
       "      <td>3.070606</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>4.722408</td>\n",
       "      <td>1.882598</td>\n",
       "      <td>5.801993</td>\n",
       "      <td>1.993536</td>\n",
       "      <td>5.432369</td>\n",
       "      <td>2.500910</td>\n",
       "      <td>9.852480</td>\n",
       "      <td>3.562274</td>\n",
       "      <td>4.493001</td>\n",
       "      <td>2.635489</td>\n",
       "      <td>...</td>\n",
       "      <td>4.174478</td>\n",
       "      <td>3.425931</td>\n",
       "      <td>7.673721</td>\n",
       "      <td>6.055497</td>\n",
       "      <td>2.052296</td>\n",
       "      <td>2.003631</td>\n",
       "      <td>3.780489</td>\n",
       "      <td>3.033883</td>\n",
       "      <td>2.414788</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SLC39A8      RHAG     DDIT4       MPO       RRM2      CCL3     TGFBI  \\\n",
       "0     7.837760  4.112866  8.046405  2.324069   4.759059  3.140721  4.143039   \n",
       "1     7.751983  1.485795  8.971049  5.258546   7.684430  2.811141  4.977650   \n",
       "2    10.279053  1.925620  8.712714  1.856427   5.890788  5.947866  4.036440   \n",
       "3     9.601609  5.888388  8.527441  5.750055   8.206514  5.134266  4.616064   \n",
       "4     7.785195  4.846312  9.301229  8.695507  10.032503  6.555199  6.542890   \n",
       "..         ...       ...       ...       ...        ...       ...       ...   \n",
       "194   7.181331  2.365467  8.184828  6.181315   7.871825  2.929050  7.998090   \n",
       "195   5.251613  2.714796  6.427088  3.523980   7.221401  2.154800  4.979873   \n",
       "196   5.905524  8.074482  6.259182  2.492298   5.146876  1.794615  8.446067   \n",
       "197   4.786756  2.279402  5.955421  8.199064   8.008226  3.352024  9.477510   \n",
       "198   4.722408  1.882598  5.801993  1.993536   5.432369  2.500910  9.852480   \n",
       "\n",
       "         MAFF      TYMS     ENPP2  ...      FEN1     AURKA     FRAT1  \\\n",
       "0    8.578116  5.230842  3.862937  ...  4.062130  3.009849  8.785775   \n",
       "1    4.715507  7.567845  3.212268  ...  4.651380  4.084429  8.823993   \n",
       "2    9.109963  6.066830  3.086101  ...  3.554817  2.921564  8.135534   \n",
       "3    5.156271  8.057016  2.684719  ...  5.330526  4.034944  7.884369   \n",
       "4    8.041716  9.813448  7.125344  ...  6.783366  6.314857  8.603358   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "194  4.431776  7.034406  3.780909  ...  4.477342  4.055716  8.839066   \n",
       "195  7.541867  7.252205  2.629904  ...  5.162441  3.987030  9.600255   \n",
       "196  8.769124  4.142922  2.246056  ...  4.168839  4.630949  7.698138   \n",
       "197  5.653967  7.454457  2.954322  ...  4.676477  4.053300  8.976000   \n",
       "198  3.562274  4.493001  2.635489  ...  4.174478  3.425931  7.673721   \n",
       "\n",
       "         SNRPG     KIF2C     POLE2     UBE2C     AURKB     CENPF  label  \n",
       "0     9.787393  2.240854  2.622261  4.787508  3.002504  2.246339      1  \n",
       "1     9.877403  3.083158  2.470562  5.049623  2.667229  3.301098      0  \n",
       "2    10.673182  2.399208  2.270243  4.136699  3.181530  2.663032      1  \n",
       "3     9.025149  3.205451  2.998327  5.846744  3.883344  3.662963      1  \n",
       "4    10.328848  4.966912  4.004817  6.864789  5.244933  4.199692      1  \n",
       "..         ...       ...       ...       ...       ...       ...    ...  \n",
       "194   9.910092  2.871094  3.078237  3.940554  3.216972  3.462929      0  \n",
       "195   6.991776  2.501947  2.859321  4.154692  3.430309  3.199781      0  \n",
       "196   9.373841  2.188718  2.145200  4.212061  2.516086  2.280285      0  \n",
       "197   6.692781  2.799296  2.676570  4.434418  3.215429  3.070606      0  \n",
       "198   6.055497  2.052296  2.003631  3.780489  3.033883  2.414788      0  \n",
       "\n",
       "[199 rows x 109 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = './data/GSE66099.csv'\n",
    "degs_file = './gene_diff_data.csv'\n",
    "# data pre-processing\n",
    "norm_filter = MinMaxScaler()\n",
    "\n",
    "label = 'Died'\n",
    "num_features = 10\n",
    "\n",
    "DEGs = pd.read_csv(degs_file)\n",
    "degs_gene_id = DEGs[\"X\"]\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "degs_gene_data = data[degs_gene_id]\n",
    "degs_gene_data.loc[:,\"label\"]  = data[label]\n",
    "degs_gene_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bab3a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data 7/3\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# y = degs_gene_data.iloc[:,108].values\n",
    "X, y = degs_gene_data.iloc[:,0:108].values, degs_gene_data.iloc[:,108].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058877a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data:\n",
    "def nml(gene_data):\n",
    "    normalize_data = (gene_data - min(gene_data))/(max(gene_data) - min(gene_data))\n",
    "    return normalize_data\n",
    "for i in range(108):\n",
    "    X[:,i] = nml(X[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c768bf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57697486, 0.34433825, 0.61513556, ..., 0.40091366, 0.20974908,\n",
       "        0.17101969],\n",
       "       [0.56674423, 0.03274116, 0.72320089, ..., 0.46379284, 0.12501523,\n",
       "        0.38131577],\n",
       "       [0.86814946, 0.08490889, 0.69300869, ..., 0.24478979, 0.25499418,\n",
       "        0.25409926],\n",
       "       ...,\n",
       "       [0.34651585, 0.81422575, 0.40625859, ..., 0.26286847, 0.08681668,\n",
       "        0.17778783],\n",
       "       [0.21307966, 0.12687091, 0.37075729, ..., 0.31621004, 0.26356147,\n",
       "        0.33536072],\n",
       "       [0.20540484, 0.07980607, 0.35282589, ..., 0.15933786, 0.21767935,\n",
       "        0.20460477]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd143cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state=1234,\n",
    "                                                   shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a2fdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 108)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07670549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# print('Training accuracy:', np.mean(rf.predict(X_train) == y_train)*100)\n",
    "# print('Test accuracy:', np.mean(rf.predict(X_test) == y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6dfd086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.01889\n",
      "Feature: 1, Score: 0.00136\n",
      "Feature: 2, Score: 0.01704\n",
      "Feature: 3, Score: 0.00836\n",
      "Feature: 4, Score: 0.00205\n",
      "Feature: 5, Score: 0.01336\n",
      "Feature: 6, Score: 0.01277\n",
      "Feature: 7, Score: 0.03794\n",
      "Feature: 8, Score: 0.00396\n",
      "Feature: 9, Score: 0.00547\n",
      "Feature: 10, Score: 0.00139\n",
      "Feature: 11, Score: 0.00075\n",
      "Feature: 12, Score: 0.00220\n",
      "Feature: 13, Score: 0.01143\n",
      "Feature: 14, Score: 0.00212\n",
      "Feature: 15, Score: 0.01397\n",
      "Feature: 16, Score: 0.01056\n",
      "Feature: 17, Score: 0.00888\n",
      "Feature: 18, Score: 0.00719\n",
      "Feature: 19, Score: 0.00285\n",
      "Feature: 20, Score: 0.00497\n",
      "Feature: 21, Score: 0.00200\n",
      "Feature: 22, Score: 0.00522\n",
      "Feature: 23, Score: 0.00854\n",
      "Feature: 24, Score: 0.00328\n",
      "Feature: 25, Score: 0.00068\n",
      "Feature: 26, Score: 0.00430\n",
      "Feature: 27, Score: 0.00720\n",
      "Feature: 28, Score: 0.02330\n",
      "Feature: 29, Score: 0.00204\n",
      "Feature: 30, Score: 0.00440\n",
      "Feature: 31, Score: 0.00499\n",
      "Feature: 32, Score: 0.00737\n",
      "Feature: 33, Score: 0.00942\n",
      "Feature: 34, Score: 0.00407\n",
      "Feature: 35, Score: 0.00000\n",
      "Feature: 36, Score: 0.00483\n",
      "Feature: 37, Score: 0.00189\n",
      "Feature: 38, Score: 0.00762\n",
      "Feature: 39, Score: 0.01984\n",
      "Feature: 40, Score: 0.01983\n",
      "Feature: 41, Score: 0.02979\n",
      "Feature: 42, Score: 0.00000\n",
      "Feature: 43, Score: 0.01174\n",
      "Feature: 44, Score: 0.00325\n",
      "Feature: 45, Score: 0.00169\n",
      "Feature: 46, Score: 0.02219\n",
      "Feature: 47, Score: 0.00234\n",
      "Feature: 48, Score: 0.05341\n",
      "Feature: 49, Score: 0.00938\n",
      "Feature: 50, Score: 0.02959\n",
      "Feature: 51, Score: 0.00484\n",
      "Feature: 52, Score: 0.00681\n",
      "Feature: 53, Score: 0.00133\n",
      "Feature: 54, Score: 0.00885\n",
      "Feature: 55, Score: 0.00510\n",
      "Feature: 56, Score: 0.01770\n",
      "Feature: 57, Score: 0.00209\n",
      "Feature: 58, Score: 0.01339\n",
      "Feature: 59, Score: 0.00465\n",
      "Feature: 60, Score: 0.00725\n",
      "Feature: 61, Score: 0.00917\n",
      "Feature: 62, Score: 0.00359\n",
      "Feature: 63, Score: 0.00522\n",
      "Feature: 64, Score: 0.02283\n",
      "Feature: 65, Score: 0.00334\n",
      "Feature: 66, Score: 0.01515\n",
      "Feature: 67, Score: 0.00962\n",
      "Feature: 68, Score: 0.00426\n",
      "Feature: 69, Score: 0.00272\n",
      "Feature: 70, Score: 0.02056\n",
      "Feature: 71, Score: 0.00825\n",
      "Feature: 72, Score: 0.00058\n",
      "Feature: 73, Score: 0.01191\n",
      "Feature: 74, Score: 0.01019\n",
      "Feature: 75, Score: 0.00204\n",
      "Feature: 76, Score: 0.00967\n",
      "Feature: 77, Score: 0.00319\n",
      "Feature: 78, Score: 0.01351\n",
      "Feature: 79, Score: 0.00980\n",
      "Feature: 80, Score: 0.00423\n",
      "Feature: 81, Score: 0.00283\n",
      "Feature: 82, Score: 0.00088\n",
      "Feature: 83, Score: 0.00278\n",
      "Feature: 84, Score: 0.05118\n",
      "Feature: 85, Score: 0.00384\n",
      "Feature: 86, Score: 0.02258\n",
      "Feature: 87, Score: 0.00424\n",
      "Feature: 88, Score: 0.02274\n",
      "Feature: 89, Score: 0.01989\n",
      "Feature: 90, Score: 0.00774\n",
      "Feature: 91, Score: 0.00105\n",
      "Feature: 92, Score: 0.02286\n",
      "Feature: 93, Score: 0.02570\n",
      "Feature: 94, Score: 0.00281\n",
      "Feature: 95, Score: 0.01353\n",
      "Feature: 96, Score: 0.00479\n",
      "Feature: 97, Score: 0.01118\n",
      "Feature: 98, Score: 0.01266\n",
      "Feature: 99, Score: 0.00103\n",
      "Feature: 100, Score: 0.00587\n",
      "Feature: 101, Score: 0.00857\n",
      "Feature: 102, Score: 0.00523\n",
      "Feature: 103, Score: 0.00240\n",
      "Feature: 104, Score: 0.00000\n",
      "Feature: 105, Score: 0.00008\n",
      "Feature: 106, Score: 0.00673\n",
      "Feature: 107, Score: 0.00326\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "importance = rf.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb1cb137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.002397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.006726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.003256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0    0.018889\n",
       "1    0.001355\n",
       "2    0.017040\n",
       "3    0.008356\n",
       "4    0.002045\n",
       "..        ...\n",
       "103  0.002397\n",
       "104  0.000000\n",
       "105  0.000080\n",
       "106  0.006726\n",
       "107  0.003256\n",
       "\n",
       "[108 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = pd.DataFrame(importance)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c62f2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_score = pd.DataFrame(degs_gene_id)\n",
    "gene_score[\"score\"] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a81b120d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SLC39A8</td>\n",
       "      <td>0.018889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RHAG</td>\n",
       "      <td>0.001355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DDIT4</td>\n",
       "      <td>0.017040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MPO</td>\n",
       "      <td>0.008356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RRM2</td>\n",
       "      <td>0.002045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>KIF2C</td>\n",
       "      <td>0.002397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>POLE2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>UBE2C</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>AURKB</td>\n",
       "      <td>0.006726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>CENPF</td>\n",
       "      <td>0.003256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X     score\n",
       "0    SLC39A8  0.018889\n",
       "1       RHAG  0.001355\n",
       "2      DDIT4  0.017040\n",
       "3        MPO  0.008356\n",
       "4       RRM2  0.002045\n",
       "..       ...       ...\n",
       "103    KIF2C  0.002397\n",
       "104    POLE2  0.000000\n",
       "105    UBE2C  0.000080\n",
       "106    AURKB  0.006726\n",
       "107    CENPF  0.003256\n",
       "\n",
       "[108 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19e61b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_score.to_csv(\"gene_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59c4a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = []\n",
    "for i in degs_gene_id:\n",
    "    gene.append(i)\n",
    "gene_rank = []\n",
    "gene_score_ind = []\n",
    "for i in range(108):\n",
    "    max_ind = np.argmax(importance)\n",
    "    gene_rank.append(gene[max_ind])\n",
    "    gene_score_ind.append(max_ind)\n",
    "    importance[max_ind] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec9e2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = gene_score_ind\n",
    "total_comb = [None]*108\n",
    "for i in range(108):\n",
    "    arr = []\n",
    "    for j in range(0, i + 1):\n",
    "        arr.append(a[j])\n",
    "    total_comb[i] = arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47ce5153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48]\n",
      "[48, 84]\n",
      "[48, 84, 7]\n",
      "[48, 84, 7, 41]\n",
      "[48, 84, 7, 41, 50]\n",
      "[48, 84, 7, 41, 50, 93]\n",
      "[48, 84, 7, 41, 50, 93, 28]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10, 1]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10, 1, 53]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10, 1, 53, 91]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10, 1, 53, 91, 99]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10, 1, 53, 91, 99, 82]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10, 1, 53, 91, 99, 82, 11]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10, 1, 53, 91, 99, 82, 11, 25]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10, 1, 53, 91, 99, 82, 11, 25, 72]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10, 1, 53, 91, 99, 82, 11, 25, 72, 105]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10, 1, 53, 91, 99, 82, 11, 25, 72, 105, 0]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10, 1, 53, 91, 99, 82, 11, 25, 72, 105, 0, 0]\n",
      "[48, 84, 7, 41, 50, 93, 28, 92, 64, 88, 86, 46, 70, 89, 39, 40, 0, 56, 2, 66, 15, 95, 78, 58, 5, 6, 98, 73, 43, 13, 97, 16, 74, 79, 76, 67, 33, 49, 61, 17, 54, 101, 23, 3, 71, 90, 38, 32, 60, 27, 18, 52, 106, 100, 9, 102, 63, 22, 55, 31, 20, 51, 36, 96, 59, 30, 26, 68, 87, 80, 34, 8, 85, 62, 65, 24, 107, 44, 77, 19, 81, 94, 83, 69, 103, 47, 12, 14, 57, 4, 29, 75, 21, 37, 45, 10, 1, 53, 91, 99, 82, 11, 25, 72, 105, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for val in total_comb:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2324bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from rdea.utils.help_functions import model_based_relevance, auc_relevance, load_DEGs\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline   \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Comment these lines if you are not using Mac OS\n",
    "import matplotlib as mpl\n",
    "# Set random seed\n",
    "seed = 8\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff601db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== INPUTS ==========\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# in and out files\n",
    "data_file = './data/GSE66099.csv'\n",
    "markers_file = './results/sepsis/fs_scores.csv'\n",
    "\n",
    "out_file = './results/sepsis/fs_scores.csv'\n",
    "is_save_results = True\n",
    "\n",
    "# data pre-processing\n",
    "is_normalize_samples = False\n",
    "is_normalize_columns = True\n",
    "norm_filter = MinMaxScaler()\n",
    "\n",
    "# experiment\n",
    "num_folds = 5\n",
    "\n",
    "label = 'Died'\n",
    "# Set random seed\n",
    "seed = 8\n",
    "np.random.seed(seed)\n",
    "\n",
    "# classifiers\n",
    "models = [\n",
    "  ('SVM-RBF',SVC(kernel='rbf',probability=True)),\n",
    "  ('RF', RandomForestClassifier(n_estimators=100,  random_state=seed)),\n",
    "  ('LR', SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=seed)),\n",
    "  ('KNN', KNeighborsClassifier(n_neighbors=15, p=2, n_jobs=-1)),\n",
    "  ('Bagging', BaggingClassifier(base_estimator=RandomForestClassifier(n_estimators=100, random_state=seed), n_estimators=10, random_state=seed)),  \n",
    "  ('Boosting', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=seed))      \n",
    " ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "998cc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate2(y_pred, y_test, y_probs):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_probs)\n",
    "    for i in range(len(y_test)):\n",
    "        if (y_pred[i]==1 and y_test[i]==1):\n",
    "            tp = tp + 1\n",
    "        elif (y_pred[i]==0 and y_test[i]==0):\n",
    "            tn = tn + 1\n",
    "        elif (y_pred[i]==1 and y_test[i]==0):\n",
    "            fp = fp + 1\n",
    "        else:\n",
    "            fn = fn + 1\n",
    "    sn = tp/(tp+fn)\n",
    "    sp = tn/(tn+fp)\n",
    "    fpr = fp/(tn+fp)\n",
    "    acc = (tp+tn)/(tp+fp+tn+fn)\n",
    "    return  np.array([acc, sn, sp, auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c88dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from scipy import interp\n",
    "from sklearn.metrics import *\n",
    "def do_cross_validation_for_sing_model(X_comb, new_y, model, num_folds=5, random_state = 0, n_runs = 5):\n",
    "   \n",
    "    res = np.zeros(shape=(num_folds * n_runs, 4)) # 5 is the number of metrics\n",
    "    count = 0\n",
    "    for cv_run in range(n_runs):\n",
    "        skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "        for train_index, test_index in skf.split(X_comb, new_y):\n",
    "            X_train2, X_test2 = X_comb[train_index], X_comb[test_index]\n",
    "            y_train2, y_test2 = new_y[train_index], new_y[test_index]\n",
    "            \n",
    "            train_filtered = X_train2\n",
    "            test_filtered = X_test2\n",
    "\n",
    "            model.fit(train_filtered, y_train2)\n",
    "            Y_probs = model.predict_proba(test_filtered)[:, 1]\n",
    "            Y_pred = model.predict(test_filtered)\n",
    "\n",
    "            res[count, :] = evaluate2(Y_pred, y_test2, Y_probs)\n",
    "            count += 1\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed630c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def std_mean_cal(res):\n",
    "    means = [None]*4\n",
    "    stds = [None]*4\n",
    "    for i in range(4):\n",
    "        mean = round(np.mean(res[:,i]),2)\n",
    "        means[i] = mean\n",
    "#         std = sum(np.square(res[:,i] - means[i]))/25\n",
    "        stds[i] = round(np.std(res[:,i]),2)\n",
    "    M = [means, stds]\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance reported using:\\t ACC\\tSn\\tSp\\tMCC\\tAUC')\n",
    "\n",
    "for model in models:\n",
    "    count = 0\n",
    "    total = []\n",
    "    for comb in total_comb:\n",
    "        X_comb = pd.DataFrame((X_test)[:,comb])\n",
    "        num_runs = 5\n",
    "        res = do_cross_validation_for_sing_model(X_comb.values, y_test, model=model[1], num_folds=5, n_runs=5)\n",
    "        result = std_mean_cal(res)\n",
    "        count += 1\n",
    "        mean_std = {\n",
    "            \"Comb\": comb,\n",
    "            \"ACC\": str((result[0])[0]) + \"+-\" + str((result[1])[0]),\n",
    "            \"Sn\": str((result[0])[1]) + \"+-\" + str((result[1])[1]),\n",
    "            \"Sp\": str((result[0])[2]) + \"+-\" + str((result[1])[2]),\n",
    "            \"AUC\": str((result[0])[3]) + \"+-\" + str((result[1])[3])\n",
    "        }\n",
    "        total.append(mean_std)\n",
    "        print(res)\n",
    "        print(count)\n",
    "        print('---------------------------------------------------------------')\n",
    "    file_name = model[0] + \"- result2.csv\"\n",
    "    (pd.DataFrame(total)).to_csv(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
